<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Red Hat simplifies transition to open source Kafka with new service registry and HTTP bridge</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/DoAIXDzFGZY/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Red Hat 3scale API Management" scheme="searchisko:content:tags" /><category term="Red Hat AMQ Streams" scheme="searchisko:content:tags" /><category term="Red Hat Integration" scheme="searchisko:content:tags" /><author><name>Hugo Guerrero</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_simplifies_transition_to_open_source_kafka_with_new_service_registry_and_http_bridge</id><updated>2019-11-26T08:00:26Z</updated><published>2019-11-26T08:00:26Z</published><content type="html">&lt;p&gt;Red Hat continues to increase the features available for users looking to implement a 100% open source, &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven architecture&lt;/a&gt; (EDA) through running &lt;a href="https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8/"&gt;Apache Kafka&lt;/a&gt; on &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt;. The &lt;a href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; Q4 release provides new features and capabilities, including ones aimed at simplifying usage and deployment of the &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;AMQ streams&lt;/a&gt; distribution of Apache Kafka. &lt;span id="more-658227"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Features available in this new version include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Service Registry as Technical Preview&lt;/li&gt; &lt;li&gt;HTTP-Kafka bridge is now generally available&lt;/li&gt; &lt;li&gt;Secure HTTP-Kafka bridge with 3scale API Management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The addition of the service registry and the HTTP Kafka bridge improves Red Hat positioning as the 100% open source platform for cloud-native Kafka workloads. These new features become a suitable complement to the usage of the Kafka Operator for OpenShift based on the &lt;a href="https://strimzi.io/2019/09/06/cncf.html"&gt;CNCF sandbox project Strimzi&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Schema service registry for event-driven architecture&lt;/h2&gt; &lt;p&gt;Red Hat Integration’s service registry, based on the &lt;a href="https://www.apicur.io/"&gt;Apicurio project&lt;/a&gt; registry, provides a way to decouple the schema used to serialize and deserialize Kafka messages with the applications that are sending/receiving them. The registry is a store for schema (and API design) artifacts providing a REST API for access management and a set of optional rules for enforcing content validity and evolution.&lt;/p&gt; &lt;p&gt;The Apicurio service registry handles the following data formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apache Avro&lt;/li&gt; &lt;li&gt;JSON Schema&lt;/li&gt; &lt;li&gt;Protobuf (protocol buffers)&lt;/li&gt; &lt;li&gt;OpenAPI&lt;/li&gt; &lt;li&gt;AsyncAPI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In addition to the registry itself, users can leverage the included custom Kafka serializers and deserializers (SerDes). These SerDes Java classes allow Kafka applications to pull relevant schemas from the Service Registry instead of requiring the schemas to be bundled with the applications.&lt;/p&gt; &lt;p&gt;Correspondingly, the registry has its own REST API to create, update, and delete artifacts as well as managing global and per-artifact rules. The registry API is compatible with another Kafka provider’s schema registry to facilitate a seamless migration to AMQ Streams as a drop-in replacement.&lt;/p&gt; &lt;p&gt;For the upcoming &lt;a href="https://access.redhat.com/support/offerings/techpreview"&gt;technical preview&lt;/a&gt;, only the Avro format will be included in the service registry for the Red Hat Integration release.&lt;/p&gt; &lt;h2&gt;Connecting to Kafka through HTTP&lt;/h2&gt; &lt;p&gt;Apache Kafka uses a custom protocol on top of TCP/IP for communication between applications and the cluster. Clients are available for many different programming languages, but there are many scenarios in which a standard protocol such as HTTP/1.1 is more appropriate.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index#assembly-kafka-bridge-overview-str"&gt;Red Hat AMQ Streams Kafka Bridge&lt;/a&gt; provides an API for integrating HTTP-based clients with a Kafka cluster running on AMQ Streams. Applications can perform typical operations such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sending messages to topics.&lt;/li&gt; &lt;li&gt;Subscribing to one or more topics.&lt;/li&gt; &lt;li&gt;Receiving messages from the subscribed topics.&lt;/li&gt; &lt;li&gt;Committing offsets related to the received messages.&lt;/li&gt; &lt;li&gt;Seeking to a specific position.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Users can deploy the Kafka Bridge into an OpenShift cluster by using the AMQ Streams Operator or similar to an AMQ Streams installation, and users can download the Kafka Bridge files for installation on Red Hat Enterprise Linux.&lt;/p&gt; &lt;p&gt;Users can provide TLS support, authentication, and authorization using the API Management capabilities of Red Hat Integration by &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index#kafka-bridge-3-scale-str"&gt;securing the Kafka Bridge with the 3scale&lt;/a&gt; component. Integration with API Management also means that additional features such as metrics, rate limiting, and billing are available.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The Red Hat Integration Q4 release makes Red Hat’s AMQ Streams distribution a better open source platform for cloud-native Kafka workloads. The Service Registry technical preview provides a common ground for data governance in the ever-changing domain of the event management bus. The general availability of the HTTP Bridge in Red Hat Integration enhances the options available to developers when building applications with Apache Kafka.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#38;linkname=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fred-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge%2F&amp;#038;title=Red%20Hat%20simplifies%20transition%20to%20open%20source%20Kafka%20with%20new%20service%20registry%20and%20HTTP%20bridge" data-a2a-url="https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/" data-a2a-title="Red Hat simplifies transition to open source Kafka with new service registry and HTTP bridge"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/"&gt;Red Hat simplifies transition to open source Kafka with new service registry and HTTP bridge&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/DoAIXDzFGZY" height="1" width="1" alt=""/&gt;</content><summary>Red Hat continues to increase the features available for users looking to implement a 100% open source, event-driven architecture (EDA) through running Apache Kafka on Red Hat OpenShift and Red Hat Enterprise Linux. The Red Hat Integration Q4 release provides new features and capabilities, including ones aimed at simplifying usage and deployment of the AMQ streams distribution of Apache Kafka.  Fe...</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2019-11-26T08:00:26Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/</feedburner:origLink></entry><entry><title>Set up Red Hat AMQ 7 custom certificates on OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/nh0YRl8hBtU/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><category term="TLS" scheme="searchisko:content:tags" /><author><name>Federico Valeri</name></author><id>searchisko:content:id:jbossorg_blog-set_up_red_hat_amq_7_custom_certificates_on_openshift</id><updated>2019-11-26T08:00:02Z</updated><published>2019-11-26T08:00:02Z</published><content type="html">&lt;p&gt;Secure communication over a computer network is one of the most important requirements for a system, and yet it can be difficult to set up correctly. This example shows how to set up &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/amq" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ 7&lt;/a&gt; end-to-end TLS encryption using a custom X.509 certificate on the &lt;a href="http://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; platform.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;p&gt;You need to have the following in place before you can proceed with this example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An OpenShift cluster up and running.&lt;/li&gt; &lt;li&gt;A custom X.509 certificate in PEM format (along with its chain).&lt;/li&gt; &lt;li&gt;An active &lt;a href="https://access.redhat.com/"&gt;Red Hat Customer Portal&lt;/a&gt; account.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span id="more-651227"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;The procedure&lt;/h2&gt; &lt;p&gt;Before we start, let&amp;#8217;s define a few handy variables:&lt;/p&gt; &lt;pre&gt;PROJECT="demo" USER="developer" BASEURL="https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/74-7.4.0.GA"&lt;/pre&gt; &lt;p&gt;The first step is to log in and create a new project to host our broker:&lt;/p&gt; &lt;pre&gt;oc login -u $USER -p x oc new-project $PROJECT&lt;/pre&gt; &lt;p&gt;Then, we need to create a dedicated ServiceAccount for deployment and add the view role:&lt;/p&gt; &lt;pre&gt;echo '{"kind": "ServiceAccount", "apiVersion": "v1", "metadata": {"name": "amq-service-account"}}' | oc create -f - oc policy add-role-to-user view system:serviceaccount:$PROJECT:amq-service-account&lt;/pre&gt; &lt;p&gt;At this point, we should have all custom certificate files available. Most likely, this signed custom certificate came from the security team, along with its private key and the whole certificate&amp;#8217;s chain (all in PEM format).&lt;/p&gt; &lt;p&gt;The certificate files consist of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;rootca.pem&lt;/strong&gt;: The root Certificate Authority (CA) in our domain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;interm.pem&lt;/strong&gt;: An intermediate CA created to sign the certificate in a specific context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;server.pem&lt;/strong&gt;: The final server certificate, which can be issued for single or multiple domains (wildcard).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;server-prk.pem&lt;/strong&gt;: The private key associated with our server&amp;#8217;s certificate.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Using these files, create a server KeyStore, convert it to Java KeyStore (JKS) format, and then trust the chain of certificates used to sign it:&lt;/p&gt; &lt;pre&gt;cat interm.pem rootca.pem &amp;#62; chain.pem cat server.pem chain.pem &amp;#62; bundle.pem openssl pkcs12 -export -in bundle.pem -inkey server-prk.pem -out server.p12 -name server -CAfile chain.pem -passout pass:secret keytool -importkeystore -alias server -srcstoretype PKCS12 -srckeystore server.p12 -srcstorepass secret -destkeystore server.jks -deststorepass secret keytool -import -noprompt -trustcacerts -alias chain -file chain.pem -keystore server.jks -storepass secret&lt;/pre&gt; &lt;p&gt;When the server KeyStore is ready, it can be imported into a Secret that must also be added to the ServiceAccount created earlier:&lt;/p&gt; &lt;pre&gt;oc create secret generic amq-app-secret --from-file=server.jks oc secrets add sa/amq-service-account secret/amq-app-secret&lt;/pre&gt; &lt;p&gt;In this example, we use the persistent AMQ 7 SSL template, because we usually want our messages to survive a broker shutdown. Let&amp;#8217;s create the image stream and download the broker&amp;#8217;s template:&lt;/p&gt; &lt;pre&gt;oc login -u system:admin oc replace --force -f $BASEURL/amq-broker-7-image-streams.yaml curl -o broker.yaml $BASEURL/templates/amq-broker-74-persistence-ssl.yaml&lt;/pre&gt; &lt;p&gt;To be able to download the broker&amp;#8217;s image from the Red Hat Container Registry, we also need to add an authentication Secret and link it to the default ServiceAccount:&lt;/p&gt; &lt;pre&gt;oc create secret docker-registry registry-auth \ --docker-server=registry.redhat.io \ --docker-username=&amp;#60;portal-username&amp;#62; \ --docker-password=&amp;#60;portal-password&amp;#62; oc secrets link default registry-auth --for=pull&lt;/pre&gt; &lt;p&gt;The next step is to deploy the broker using the downloaded template and pass our KeyStore as a parameter:&lt;/p&gt; &lt;pre&gt;oc login -u $USER -p x oc process -f broker.yaml \ -p APPLICATION_NAME=$PROJECT \ -p AMQ_USER=admin \ -p AMQ_PASSWORD=admin \ -p AMQ_TRUSTSTORE=server.jks \ -p AMQ_TRUSTSTORE_PASSWORD=secret \ -p AMQ_KEYSTORE=server.jks \ -p AMQ_KEYSTORE_PASSWORD=secret \ | oc create -f -&lt;/pre&gt; &lt;p&gt;We are almost finished. The last step is to create a service and a passthrough route to expose the desired port to the external world. Here we are exposing the AMQP port, but you can do the same with the other available protocols:&lt;/p&gt; &lt;pre&gt;oc create -f - &amp;#60;&amp;#60;EOF apiVersion: v1 kind: Service metadata: labels: application: $PROJECT-amq name: broker-amq-amqp-ssl spec: ports: - port: 5671 targetPort: 5671 selector: statefulset.kubernetes.io/pod-name: $PROJECT-amq-0 EOF oc create route passthrough --service=broker-amq-amqp-ssl&lt;/pre&gt; &lt;h2&gt;Java client setup&lt;/h2&gt; &lt;p&gt;You can use your preferred JMS library to build your client, but you will certainly need a truststore in JKS format for one-way TLS authentication:&lt;/p&gt; &lt;pre&gt;keytool -import -noprompt -file server.pem -alias server -keystore truststore.jks -storepass secret&lt;/pre&gt; &lt;p&gt;If you want to access the broker from outside OpenShift, then you also need to use a ConnectionFactory URL similar to this one:&lt;/p&gt; &lt;pre&gt;amqps://broker-amq-amqp-ssl-demo.192.168.64.53.nip.io:443?transport.verifyHost=false&amp;#38;transport.trustStoreLocation=src/main/resources/truststore.jks&amp;#38;transport.trustStorePassword=secret&lt;/pre&gt; &lt;h3&gt;Additional notes&lt;/h3&gt; &lt;p&gt;You must bind the hostname you are using in the certificate&amp;#8217;s CN field to the cluster&amp;#8217;s HAProxy IP address in your DNS server. If you are using a homemade CA, then you also need to trust the chain on the client machine to access the Hawtio web console.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F26%2Fset-up-red-hat-amq-7-custom-certificates-on-openshift%2F&amp;#038;title=Set%20up%20Red%20Hat%20AMQ%207%20custom%20certificates%20on%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2019/11/26/set-up-red-hat-amq-7-custom-certificates-on-openshift/" data-a2a-title="Set up Red Hat AMQ 7 custom certificates on OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/26/set-up-red-hat-amq-7-custom-certificates-on-openshift/"&gt;Set up Red Hat AMQ 7 custom certificates on OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/nh0YRl8hBtU" height="1" width="1" alt=""/&gt;</content><summary>Secure communication over a computer network is one of the most important requirements for a system, and yet it can be difficult to set up correctly. This example shows how to set up Red Hat AMQ 7 end-to-end TLS encryption using a custom X.509 certificate on the Red Hat OpenShift platform. Prerequisites You need to have the following in place before you can proceed with this example: An OpenShift ...</summary><dc:creator>Federico Valeri</dc:creator><dc:date>2019-11-26T08:00:02Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/26/set-up-red-hat-amq-7-custom-certificates-on-openshift/</feedburner:origLink></entry><entry><title>Infinispan Operator 1.0.1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_MvBfrHQ-8M/" /><category term="dev-preview" scheme="searchisko:content:tags" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><author><name>Vittorio Rigamonti</name></author><id>searchisko:content:id:jbossorg_blog-infinispan_operator_1_0_1</id><updated>2019-11-26T10:50:13Z</updated><published>2019-11-25T12:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Dear Infinispan community,&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;we know you are happy with the new shining 10.0.0 Infinispan release, but if you are among those who are missing a new operator version for safely running your Infinispan Chupachabra in the clound, this post is for you!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_versioning_and_channels"&gt;&lt;a class="anchor" href="#_versioning_and_channels"&gt;&lt;/a&gt;Versioning and channels&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This is our first blog post about 1.0.x operator series (yeah, sorry 1.0.0 we forgot about you) and as you can notice there’s no Alpha, Beta or CR label at the end of the release tag. This is because OperatorHub and Openshift Catalog only allow numerical version like Maj.Min.Mic and instead of labels we now use the channel to indicate the stability of a release. We have 2 live channels at the moment for the Infinispan operator: &lt;code&gt;stable&lt;/code&gt; and &lt;code&gt;dev-preview&lt;/code&gt;. Current &lt;code&gt;stable&lt;/code&gt; is 0.3.2 which is for the 9.x Infinispan cluster and current &lt;code&gt;dev-preview&lt;/code&gt; is 1.0.1 which works with 10.x clusters.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_new_features"&gt;&lt;a class="anchor" href="#_new_features"&gt;&lt;/a&gt;New features&lt;/h3&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;New Infinispan image configuration: we cleaned up the image configuration process: instead of rely on a large set of env variables, now the operator configures the Infinispan image via a single .yaml file.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Container configurability: CR .yaml file lets you configure memory and CPU (and also extras Java opts) assigned to the container;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Encryption: TLS can be setup providing TLS certificates or using platform service as the Openshift seriving certs service (TLS will be on by default in the next release);&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;We now have some good docs: &lt;a href="https://infinispan.org/infinispan-operator/master/operator.html" class="bare"&gt;https://infinispan.org/infinispan-operator/master/operator.html&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Project README has been also improved: &lt;a href="https://github.com/infinispan/infinispan-operator/blob/1.0.1/README.md" class="bare"&gt;https://github.com/infinispan/infinispan-operator/blob/1.0.1/README.md&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_get_it"&gt;&lt;a class="anchor" href="#_get_it"&gt;&lt;/a&gt;Get it&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Infinispan Operator 1.0.1 works on Kind/Kubernetes 1.16 (CI) and Openshift 3.11, 4.x (developed on). You can install it:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;manually, follow the README;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;with OLM on Kubernetes, &lt;a href="https://operatorhub.io/operator/infinispan/dev-preview/infinispan-operator.v1.0.0" class="bare"&gt;https://operatorhub.io/operator/infinispan/dev-preview/infinispan-operator.v1.0.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;with OLM from the Openshift Operator Catalog&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;And remember: it’s a dev-preview release, you can have a lot of fun with it!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_contribute"&gt;&lt;a class="anchor" href="#_contribute"&gt;&lt;/a&gt;Contribute&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As usual source code is open at: &lt;a href="https://github.com/infinispan/infinispan-operator" class="bare"&gt;https://github.com/infinispan/infinispan-operator&lt;/a&gt;. You can see what’s going on, comment the code or the new pull requests, ask for new features and also develop them!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Thanks for following us, Infinispan&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_MvBfrHQ-8M" height="1" width="1" alt=""/&gt;</content><summary>Dear Infinispan community, we know you are happy with the new shining 10.0.0 Infinispan release, but if you are among those who are missing a new operator version for safely running your Infinispan Chupachabra in the clound, this post is for you! Versioning and channels This is our first blog post about 1.0.x operator series (yeah, sorry 1.0.0 we forgot about you) and as you can notice there’s no ...</summary><dc:creator>Vittorio Rigamonti</dc:creator><dc:date>2019-11-25T12:00:00Z</dc:date><feedburner:origLink>http://infinispan.org/blog/2019/11/25/infinispan-operator-1/</feedburner:origLink></entry><entry><title>Kogito tooling for friendly DMN and BPMN visualization on GitHub</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/y10ynmrm-aI/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="kogito" scheme="searchisko:content:tags" /><category term="Red Hat BPM Suite" scheme="searchisko:content:tags" /><author><name>Karina Varela</name></author><id>searchisko:content:id:jbossorg_blog-kogito_tooling_for_friendly_dmn_and_bpmn_visualization_on_github</id><updated>2019-11-25T08:00:51Z</updated><published>2019-11-25T08:00:51Z</published><content type="html">&lt;p&gt;Business automation is highly dependent on the development of business processes and rules that are easily understood by anyone involved with the project. To achieve this goal, the &lt;a href="https://kogito.kie.org/"&gt;KIE and Kogito&lt;/a&gt; teams choose to adopt the usage of patterns defined by &lt;a href="https://en.wikipedia.org/wiki/Object_Management_Group"&gt;OMG&lt;/a&gt; as the triple crown of process management:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DMN&lt;/strong&gt; — Decision Model and Notation&lt;/li&gt; &lt;li&gt;&lt;strong&gt; BPMN&lt;/strong&gt; — Business Process Model and Notation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CMMN&lt;/strong&gt; — Case Management Model and Notation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this new &lt;a href="https://medium.com/kie-foundation/bpmn-chrome-extension-released-alpha-abe89676d76"&gt;BPMN and DMN Chrome extension&lt;/a&gt;, you can finally see business processes and rules directly on GitHub.&lt;/p&gt; &lt;p&gt;If you work with business automation projects, you are probably familiar with these standards. Additionally, you&amp;#8217;ve probably been frustrated by facing a huge XML file while trying to see the latest changes or checking a business asset on GitHub. If that is your case, you will probably love this Chrome extension: It allows users to see business processes and business rules versioned as BPMN and DMN files directly on the GitHub webpage.&lt;span id="more-649817"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;How this helps&lt;/h2&gt; &lt;p&gt;If you are curious about this extension and how it helps, see in Figure 1 how a BPMN file is displayed in GitHub by default, and then see in Figure 2 how this same file is displayed with the BPMN and DMN Chrome extension. If you feel like trying this extension out yourself, keep reading.&lt;/p&gt; &lt;div id="attachment_650897" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-650897" class="wp-image-650897 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.05.37-1024x783.png" alt="" width="640" height="489" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.05.37-1024x783.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.05.37-300x229.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.05.37-768x587.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.05.37.png 1050w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-650897" class="wp-caption-text"&gt;Figure 1: A BPMN file is displayed in GitHub, without the extension.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_650917" style="width: 410px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-650917" class="wp-image-650917" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.08.40-300x245.png" alt="" width="400" height="326" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.08.40-300x245.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.08.40-768x627.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.08.40.png 1022w" sizes="(max-width: 400px) 100vw, 400px" /&gt;&lt;p id="caption-attachment-650917" class="wp-caption-text"&gt;Figure 2: A BPMN file displayed with the BPMN and DMN Chrome extension.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;How to install&lt;/h2&gt; &lt;p&gt;This tool is still not available in the Chrome store, but you can already try it out and take advantage of its benefits. To have this extension working on your Chrome browser, follow these simple steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Access the &lt;a href="https://github.com/kiegroup/kogito-tooling/releases" target="_blank" rel="noopener noreferrer"&gt;Kogito Tooling release pages&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Look for the latest release and click the link, then scroll down to the end of the release notes.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;Assets&lt;/em&gt; to expand the menu and see the available options, as shown in Figure 3:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_650927" style="width: 410px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-650927" class="wp-image-650927" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.15.08-300x78.png" alt="" width="400" height="103" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.15.08-300x78.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.15.08-768x199.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.15.08.png 812w" sizes="(max-width: 400px) 100vw, 400px" /&gt;&lt;p id="caption-attachment-650927" class="wp-caption-text"&gt;Figure 3: Open the Assets menu to find the Chrome extension.&lt;/p&gt;&lt;/div&gt; &lt;ol start="4"&gt; &lt;li&gt;Download and unzip the available ZIP file.&lt;/li&gt; &lt;li&gt;In Chrome, go to the upper right corner and then access and activate the Developer mode, as shown in Figure 4:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_650947" style="width: 606px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-650947" class="wp-image-650947 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.17.16.png" alt="" width="596" height="119" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.17.16.png 596w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.17.16-300x60.png 300w" sizes="(max-width: 596px) 100vw, 596px" /&gt;&lt;p id="caption-attachment-650947" class="wp-caption-text"&gt;Figure 4: Turn on Developer mode.&lt;/p&gt;&lt;/div&gt; &lt;ol start="6"&gt; &lt;li&gt;Click on &lt;em&gt;Load unpacked&lt;/em&gt; button and choose the directory you just unzipped, as shown in Figure 5:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_650957" style="width: 444px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-650957" class="wp-image-650957 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.20.22.png" alt="" width="434" height="229" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.20.22.png 434w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-06-at-23.20.22-300x158.png 300w" sizes="(max-width: 434px) 100vw, 434px" /&gt;&lt;p id="caption-attachment-650957" class="wp-caption-text"&gt;Figure 5: Open the new directory.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;That is it. You should now see the BPMN and DMN extension available for use. If you want to try it out, access a business asset on GitHub (e.g., &lt;a href="https://github.com/kiegroup/kogito-examples/blob/master/onboarding-example/onboarding/src/main/resources/org/kie/kogito/examples/onboarding/onboarding.bpmn" target="_blank" rel="noopener noreferrer"&gt;this one&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We hope you enjoy this plugin developed by the KIE Group. To learn more about &lt;a href="https://kogito.kie.org"&gt;Kogito&lt;/a&gt;, check out these related articles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/08/29/kogito-for-quarkus-intelligent-applications/"&gt;Kogito for Quarkus intelligent applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/08/29/create-your-first-application-with-kogito/"&gt;Create your first application with Kogito&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/09/23/devnation-live-event-driven-business-automation-powered-by-cloud-native-java/"&gt;Event-driven business automation powered by cloud-native Java&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/07/23/devnation-live-introducing-kogito/"&gt;DevNation Live: Introducing Kogito&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#38;linkname=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fkogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github%2F&amp;#038;title=Kogito%20tooling%20for%20friendly%20DMN%20and%20BPMN%20visualization%20on%20GitHub" data-a2a-url="https://developers.redhat.com/blog/2019/11/25/kogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github/" data-a2a-title="Kogito tooling for friendly DMN and BPMN visualization on GitHub"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/25/kogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github/"&gt;Kogito tooling for friendly DMN and BPMN visualization on GitHub&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/y10ynmrm-aI" height="1" width="1" alt=""/&gt;</content><summary>Business automation is highly dependent on the development of business processes and rules that are easily understood by anyone involved with the project. To achieve this goal, the KIE and Kogito teams choose to adopt the usage of patterns defined by OMG as the triple crown of process management: DMN — Decision Model and Notation BPMN — Business Process Model and Notation CMMN — Case Management Mo...</summary><dc:creator>Karina Varela</dc:creator><dc:date>2019-11-25T08:00:51Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/25/kogito-tooling-for-friendly-dmn-and-bpmn-visualization-on-github/</feedburner:origLink></entry><entry><title>Getting started with .NET Core in Red Hat Enterprise Linux 8.1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5aL4PMZNo8E/" /><category term=".NET Core" scheme="searchisko:content:tags" /><category term="feature" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat Enterprise Linux" scheme="searchisko:content:tags" /><author><name>omajid@redhat.com</name></author><id>searchisko:content:id:jbossorg_blog-getting_started_with_net_core_in_red_hat_enterprise_linux_8_1</id><updated>2019-11-25T08:00:18Z</updated><published>2019-11-25T08:00:18Z</published><content type="html">&lt;p&gt;One exciting feature in the recent release of Red Hat Enterprise Linux 8.1 is &lt;a href="https://developers.redhat.com/blog/2019/10/17/new-features-in-net-core-3-0-on-linux/"&gt;.NET Core 3.0&lt;/a&gt;. In this article, we will take a quick look at using .NET Core on &lt;a href="https://developers.redhat.com/rhel8/"&gt;Red Hat Enterprise Linux 8&lt;/a&gt;. We will cover installing .NET Core RPMs and using the RHEL-based &lt;a href="”https://developers.redhat.com/blog/tag/ubi/”" target="”_blank”" rel="noopener noreferrer"&gt;Universal Base Image&lt;/a&gt; container images.&lt;/p&gt; &lt;h2&gt;Installing .NET Core packages on RHEL 8&lt;/h2&gt; &lt;p&gt;With &lt;a href="https://developers.redhat.com/blog/2019/05/07/red-hat-enterprise-linux-8-developer-cheat-sheet/"&gt;RHEL 8&lt;/a&gt;, .NET Core is included in the &lt;a href="”https://developers.redhat.com/blog/2018/11/15/rhel8-introducing-appstreams/”" target="”_blank”" rel="noopener noreferrer"&gt;AppStream repositories&lt;/a&gt;, which are enabled by default on RHEL 8 systems. At least two versions of &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET Core&lt;/a&gt; are already available on RHEL 8, and more will be added as they are released.&lt;span id="more-652767"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Multiple versions of .NET Core can be installed in parallel (side-by-side). You can pick and choose which components of .NET Core (SDK, Runtime) you need and install just those. Installing a component will install all of its dependencies. For example, installing a .NET Core SDK will also install the corresponding .NET Core Runtime as well as any other additional SDK dependencies.&lt;/p&gt; &lt;p&gt;You can install a specific version of the .NET Core SDK:&lt;/p&gt; &lt;pre&gt;dnf install dotnet-sdk-2.1 &lt;/pre&gt; &lt;p&gt;Or&lt;/p&gt; &lt;pre&gt;dnf install dotnet-sdk-3.0 &lt;/pre&gt; &lt;p&gt;In general, you can install the .NET Core SDK version &lt;code&gt;x.y&lt;/code&gt; using:&lt;/p&gt; &lt;pre&gt;dnf install dotnet-sdk-x.y &lt;/pre&gt; &lt;p&gt;If you are not interested in developing .NET Core applications, rather just running them, you can skip the SDK and only install a specific version of the .NET Core Runtime. For example:&lt;/p&gt; &lt;pre&gt; dnf install dotnet-runtime-2.1 &lt;/pre&gt; &lt;p&gt;Or&lt;/p&gt; &lt;pre&gt; dnf install dotnet-runtime-3.0 &lt;/pre&gt; &lt;p&gt;Generally, you can install the .NET Core Runtime version &lt;code&gt;x.y&lt;/code&gt; using:&lt;/p&gt; &lt;pre&gt;dnf install dotnet-runtime-x.y &lt;/pre&gt; &lt;p&gt;Starting with .NET Core 3.0, you also can install the ASP.NET Core Runtime, which lets you run framework-dependent ASP.NET Core applications:&lt;/p&gt; &lt;pre&gt;dnf install aspnetcore-runtime-3.0 &lt;/pre&gt; &lt;h2&gt;Running .NET Core&lt;/h2&gt; &lt;p&gt;Once you have installed .NET Core on RHEL 8, you can simply start using the &lt;code&gt;dotnet&lt;/code&gt; command. To make sure .NET Core is installed, try:&lt;/p&gt; &lt;pre&gt; dotnet --info &lt;/pre&gt; &lt;p&gt;That should show more information about .NET Core, including the specific components that are installed:&lt;/p&gt; &lt;pre&gt;.NET Core SDK (reflecting any global.json): Version: 3.0.100 Commit: 04339c3a26 Runtime Environment: OS Name: rhel OS Version: 8 OS Platform: Linux RID: rhel.8-x64 Base Path: /usr/lib64/dotnet/sdk/3.0.100/ Host (useful for support): Version: 3.0.0 Commit: 7d57652f33 .NET Core SDKs installed: 2.1.509 [/usr/lib64/dotnet/sdk] 3.0.100 [/usr/lib64/dotnet/sdk] .NET Core runtimes installed: Microsoft.AspNetCore.App 3.0.0 [/usr/lib64/dotnet/shared/Microsoft.AspNetCore.App] Microsoft.NETCore.App 2.1.13 [/usr/lib64/dotnet/shared/Microsoft.NETCore.App] Microsoft.NETCore.App 3.0.0 [/usr/lib64/dotnet/shared/Microsoft.NETCore.App] To install additional .NET Core runtimes or SDKs: https://aka.ms/dotnet-download &lt;/pre&gt; &lt;p&gt;We can now use .NET Core SDK to create, build, publish, and run a simple Hello World application:&lt;/p&gt; &lt;pre&gt;$ mkdir HelloWorld $ cd HelloWorld/ $ dotnet new console Welcome to .NET Core 3.0! --------------------- SDK Version: 3.0.100 ---------------- Explore documentation: https://aka.ms/dotnet-docs Report issues and find source on GitHub: https://github.com/dotnet/core Find out what's new: https://aka.ms/dotnet-whats-new Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https Use 'dotnet --help' to see available commands or visit: https://aka.ms/dotnet-cli-docs Write your first app: https://aka.ms/first-net-core-app -------------------------------------------------------------------------------------- Getting ready... The template "Console Application" was created successfully. Processing post-creation actions... Running 'dotnet restore' on /HelloWorld/HelloWorld.csproj... Restore completed in 50.91 ms for /HelloWorld/HelloWorld.csproj. Restore succeeded. $ dotnet publish --configuration Release --runtime rhel.8-x64 --self-contained false Microsoft (R) Build Engine version 16.3.0+0f4c62fea for .NET Core Copyright (C) Microsoft Corporation. All rights reserved. Restore completed in 49.95 ms for /HelloWorld/HelloWorld.csproj. HelloWorld -&amp;#62; /HelloWorld/bin/Release/netcoreapp3.0/rhel.8-x64/HelloWorld.dll HelloWorld -&amp;#62; /HelloWorld/bin/Release/netcoreapp3.0/rhel.8-x64/publish/ $ dotnet bin/Release/netcoreapp3.0/rhel.8-x64/publish/HelloWorld.dll Hello World! &lt;/pre&gt; &lt;p&gt;See the &lt;a href="”https://docs.microsoft.com/en-us/dotnet/core/”" target="”_blank”" rel="noopener noreferrer"&gt;.NET Core documentation&lt;/a&gt; for more information, including references, samples, and tutorials.&lt;/p&gt; &lt;h2&gt;Using .NET Core RHEL 8-based container images&lt;/h2&gt; &lt;p&gt;With &lt;a href="https://developers.redhat.com/rhel8/"&gt;Red Hat Enterprise Linux 8&lt;/a&gt;, .NET Core also is available in RHEL 8-based container images called Universal Base Image. You can use the container images to develop and deploy your .NET Core applications in containerized environments, such as &lt;a href="”https://developers.redhat.com/products/openshift/overview”" target="”_blank”" rel="noopener noreferrer"&gt;OpenShift&lt;/a&gt; and &lt;a href="”https://developers.redhat.com/topics/kubernetes/”" target="”_blank”" rel="noopener noreferrer"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To get a .NET Core 2.1 SDK container, use:&lt;/p&gt; &lt;pre&gt;registry.access.redhat.com/ubi8/dotnet-21 &lt;/pre&gt; &lt;p&gt;To get a .NET Core 3.0 SDK container, use:&lt;/p&gt; &lt;pre&gt;registry.access.redhat.com/ubi8/dotnet-30 &lt;/pre&gt; &lt;p&gt;To get a .NET Core 2.1 runtime container, use:&lt;/p&gt; &lt;pre&gt;registry.access.redhat.com/ubi8/dotnet-21-runtime &lt;/pre&gt; &lt;p&gt;To get a .NET Core 3.0 runtime container, use:&lt;/p&gt; &lt;pre&gt;registry.access.redhat.com/ubi8/dotnet-30-runtime &lt;/pre&gt; &lt;h2&gt;Running .NET Core containers&lt;/h2&gt; &lt;p&gt;You can use the .NET Core RHEL 8-based containers in your container pipelines. You can use it for deployment to cloud environments. You can also use it for building source-to-image (also called s2i) applications.&lt;/p&gt; &lt;p&gt;As an example, let’s create, build, and run a Hello World-style application in a container. Create a &lt;code&gt;Dockerfile&lt;/code&gt; that contains the following:&lt;/p&gt; &lt;pre&gt;FROM registry.access.redhat.com/ubi8/dotnet-30 RUN dotnet --info &amp;#38;&amp;#38; \ dotnet new console -o HelloWorld &amp;#38;&amp;#38; \ cd HelloWorld &amp;#38;&amp;#38; \ dotnet publish --configuration Release ENTRYPOINT dotnet HelloWorld/bin/Release/netcoreapp3.0/publish/HelloWorld.dll &lt;/pre&gt; &lt;p&gt;You can build and run this using podman or docker commands:&lt;/p&gt; &lt;pre&gt;$ podman build -t hello . STEP 1: FROM registry.access.redhat.com/ubi8/dotnet-30 Getting image source signatures Copying blob d4639cd2c710 done Copying blob 1fa946e8e839 done Copying blob 340ff6d7f58c done Copying blob 0e8ea260d026 done Copying config ca818f6208 done Writing manifest to image destination Storing signatures STEP 2: RUN dotnet --info &amp;#38;&amp;#38; dotnet new console -o HelloWorld &amp;#38;&amp;#38; cd HelloWorld &amp;#38;&amp;#38; dotnet publish --configuration Release .NET Core SDK (reflecting any global.json): Version: 3.0.100 Commit: 04339c3a26 Runtime Environment: OS Name: rhel OS Version: 8 OS Platform: Linux RID: rhel.8-x64 Base Path: /usr/lib64/dotnet/sdk/3.0.100/ Host (useful for support): Version: 3.0.0 Commit: 7d57652f33 .NET Core SDKs installed: 3.0.100 [/usr/lib64/dotnet/sdk] .NET Core runtimes installed: Microsoft.AspNetCore.App 3.0.0 [/usr/lib64/dotnet/shared/Microsoft.AspNetCore.App] Microsoft.NETCore.App 3.0.0 [/usr/lib64/dotnet/shared/Microsoft.NETCore.App] To install additional .NET Core runtimes or SDKs: https://aka.ms/dotnet-download Getting ready... The template "Console Application" was created successfully. Processing post-creation actions... Running 'dotnet restore' on HelloWorld/HelloWorld.csproj... Restore completed in 51.76 ms for /opt/app-root/src/HelloWorld/HelloWorld.csproj. Restore succeeded. Microsoft (R) Build Engine version 16.3.0+0f4c62fea for .NET Core Copyright (C) Microsoft Corporation. All rights reserved. Restore completed in 14.11 ms for /opt/app-root/src/HelloWorld/HelloWorld.csproj. HelloWorld -&amp;#62; /opt/app-root/src/HelloWorld/bin/Release/netcoreapp3.0/HelloWorld.dll HelloWorld -&amp;#62; /opt/app-root/src/HelloWorld/bin/Release/netcoreapp3.0/publish/ 164ce90cf7892154ab5a6a00f8d5f890dd26ffaabe0b6dd58c4005603fe325d4 STEP 3: ENTRYPOINT dotnet HelloWorld/bin/Release/netcoreapp3.0/publish/HelloWorld.dll STEP 4: COMMIT hello 1bd4ceedbcec15de4d99ee79ad5860d6f97031da1f4db32f7d408274262f8bec $ podman run -it hello Hello World! &lt;/pre&gt; &lt;h2&gt;Support and lifecycle&lt;/h2&gt; &lt;p&gt;In general, .NET Core on Red Hat Enterprise Linux tries to follow the &lt;a href="”https://dotnet.microsoft.com/platform/support/policy/dotnet-core”" target="”_blank”" rel="noopener noreferrer"&gt;.NET Core lifecycle established by Microsoft&lt;/a&gt;. To learn more about .NET Core lifecycle on RHEL 8, visit the &lt;a href="”https://access.redhat.com/support/policy/updates/rhel8-app-streams-life-cycle”" target="”_blank”" rel="noopener noreferrer"&gt;Red Hat Enterprise Linux 8 Application Streams Life Cycle&lt;/a&gt; page.&lt;/p&gt; &lt;p&gt;If you run into any errors, please &lt;a href="”https://bugzilla.redhat.com/enter_bug.cgi?product=Red%20Hat%20Enterprise%20Linux%208&amp;#38;component=dotnet”" target="”_blank”" rel="noopener noreferrer"&gt;report it as a bug in Bugzilla&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#38;linkname=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F25%2Fgetting-started-with-net-core-in-red-hat-enterprise-linux-8-1%2F&amp;#038;title=Getting%20started%20with%20.NET%20Core%20in%20Red%20Hat%20Enterprise%20Linux%208.1" data-a2a-url="https://developers.redhat.com/blog/2019/11/25/getting-started-with-net-core-in-red-hat-enterprise-linux-8-1/" data-a2a-title="Getting started with .NET Core in Red Hat Enterprise Linux 8.1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/25/getting-started-with-net-core-in-red-hat-enterprise-linux-8-1/"&gt;Getting started with .NET Core in Red Hat Enterprise Linux 8.1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5aL4PMZNo8E" height="1" width="1" alt=""/&gt;</content><summary>One exciting feature in the recent release of Red Hat Enterprise Linux 8.1 is .NET Core 3.0. In this article, we will take a quick look at using .NET Core on Red Hat Enterprise Linux 8. We will cover installing .NET Core RPMs and using the RHEL-based Universal Base Image container images. Installing .NET Core packages on RHEL 8 With RHEL 8, .NET Core is included in the AppStream repositories, whic...</summary><dc:creator>omajid@redhat.com</dc:creator><dc:date>2019-11-25T08:00:18Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/25/getting-started-with-net-core-in-red-hat-enterprise-linux-8-1/</feedburner:origLink></entry><entry><title>Voxxed Days Milan 2020 - Open Career, Microservice Questions, and DevOps Heroes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zshxhWKkMqc/voxxed-days-milan-2020-open-career-microservice-questions-devops-heroes.html" /><category term="AppDev" scheme="searchisko:content:tags" /><category term="Automate" scheme="searchisko:content:tags" /><category term="conference" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="FUSE" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="Process Automation Manager" scheme="searchisko:content:tags" /><category term="workshops" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-voxxed_days_milan_2020_open_career_microservice_questions_and_devops_heroes</id><updated>2019-11-25T09:31:25Z</updated><published>2019-11-25T06:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-MoUUJCrGpeE/XdubNVK78EI/AAAAAAAAwvA/my1eyzVW_sABLEpISOicuNQNhHkn5jtHgCNcBGAsYHQ/s1600/Screenshot%2B2019-11-25%2Bat%2B10.06.53.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img alt="voxxed days milan" border="0" data-original-height="606" data-original-width="1576" height="123" src="https://1.bp.blogspot.com/-MoUUJCrGpeE/XdubNVK78EI/AAAAAAAAwvA/my1eyzVW_sABLEpISOicuNQNhHkn5jtHgCNcBGAsYHQ/s320/Screenshot%2B2019-11-25%2Bat%2B10.06.53.png" title="" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;The usual process is to wait for the &lt;a href="https://voxxeddays.com/milan/" target="_blank"&gt;conference call for papers&lt;/a&gt; (Voxxed Days Milan) to close before publishing the sessions and workshops submitted, but this time it's going to be promotion of the event above the talks submitted!&lt;br /&gt;&lt;br /&gt;&lt;a href="https://voxxeddays.com/milan/" target="_blank"&gt;Voxxed Days Milan 2020&lt;/a&gt; shall be hosted on a single day, May 16th 2020 in Milan, Italy. The organizers reached out for some help publicizing their event and getting you, the great speakers in our developer communities, motivated to submit your ideas for &lt;a href="https://voxxeddays.com/milan/" target="_blank"&gt;Voxxed Days Milan&lt;/a&gt; call for papers.&lt;br /&gt;&lt;br /&gt;It's worth noting, any talks you end up giving are going to be recorded. After the conference all videos of the talks will eventually be published online. You can enjoy previous’ year edition of Voxxed Days Milan 2019 recordings &lt;a href="https://www.youtube.com/playlist?list=PLRsbF2sD7JVpfbaiBoeCOCMCWxSTuuZ5c"&gt;here&lt;/a&gt;. Even those not at Voxxed Days Milan can watch them, but the real fun was being there.&lt;br /&gt;&lt;br /&gt;I'll admit, I got a bit excited as the event looks to be great fun and in a wonderful city full of history and vibrant life. I might have gone a bit overboard but submitted no less than three ideas; one is a workshop overview, one is an insightful look at integration challenges, and the final one is a keynote on how open can be the key to your career.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;Below my submissions to Voxxed Days Milan 2020:&lt;br /&gt;&lt;br /&gt;&lt;h3 style="background-color: white; color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; margin: 0px; position: relative;"&gt;DevOps Heroes: Adding automation integration to your toolbox&lt;/h3&gt;&lt;i style="background-color: white; color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;"&gt;DevOps is more than the process of automating your CI/CD pipelines to generate code and deployment artifacts. It's also about organizational change and integration of many subtle processes that help you to deliver and manage applications seamlessly. Let's unlock the power of process integration in a live coding session. We'll install the tooling, build the project components, and deploy a DevOps testing automation integration project right in front of your eyes. Best of all, everything is freely available in an online self-paced workshop format to pursue at home. Join me for some DevOps fun!&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3 style="background-color: white; color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; margin: 0px; position: relative;"&gt;5 Questions Everyone Ignores with Microservices&lt;/h3&gt;&lt;i style="background-color: white; color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;"&gt;The daily hype is all around you. Microservices are a necessary step along the path to integration for a digitally successful future for your organization. Funny enough, when discussing the development impact while transitioning to microservices, there are five questions that keep popping up. This session covers the questions that everyone should ask about microservices. Join us for an hour of power, where real life developer experiences are used to highlight the lessons we're all learning as we transition our integration infrastructure into modern day microservices.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3 style="background-color: white; color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; margin: 0px; position: relative;"&gt;Open Key to Your Career&lt;/h3&gt;&lt;i style="background-color: white; color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;"&gt;It's not coincidence. It's not luck. It's not going to happen by itself, so what's the secret sauce? Understanding what makes a career in open source grow, what choices are crucial, and what actions accelerate or damage your open source future are sometimes hard to grasp. Learning to position, expand and grow your personal brand in the open source world is what this session provides. Be ready for your next step in open source. Join me for an hour of power where you'll be given a clear and easy to use plan for jump starting your open source career immediately.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Fingers crossed and I'll see you there!&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ka14bE5tQCQ:GY--0cC4m9Y:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ka14bE5tQCQ:GY--0cC4m9Y:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ka14bE5tQCQ:GY--0cC4m9Y:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ka14bE5tQCQ:GY--0cC4m9Y:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ka14bE5tQCQ:GY--0cC4m9Y:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/ka14bE5tQCQ" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zshxhWKkMqc" height="1" width="1" alt=""/&gt;</content><summary>The usual process is to wait for the conference call for papers (Voxxed Days Milan) to close before publishing the sessions and workshops submitted, but this time it's going to be promotion of the event above the talks submitted! Voxxed Days Milan 2020 shall be hosted on a single day, May 16th 2020 in Milan, Italy. The organizers reached out for some help publicizing their event and getting you, t...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2019-11-25T06:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/ka14bE5tQCQ/voxxed-days-milan-2020-open-career-microservice-questions-devops-heroes.html</feedburner:origLink></entry><entry><title>Red Hat advances Debezium CDC connectors for Apache Kafka support to Technical Preview</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/K4vla7dkDoQ/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="debezium" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><author><name>Hugo Guerrero</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_advances_debezium_cdc_connectors_for_apache_kafka_support_to_technical_preview</id><updated>2019-11-22T08:05:18Z</updated><published>2019-11-22T08:05:18Z</published><content type="html">&lt;p&gt;&lt;span style="font-weight: 400;"&gt;After a couple of months in Developer Preview, the Debezium &lt;/span&gt;&lt;a href="https://www.redhat.com/en/topics/integration/what-is-apache-kafka"&gt;&lt;span style="font-weight: 400;"&gt;Apache Kafka&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; connectors for change data capture (CDC) are now available as a &lt;/span&gt;&lt;a href="https://access.redhat.com/support/offerings/techpreview"&gt;&lt;span style="font-weight: 400;"&gt;Technical Preview&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; as part of the Q4 release of &lt;/span&gt;&lt;a href="https://www.redhat.com/en/products/integration"&gt;&lt;span style="font-weight: 400;"&gt;Red Hat Integration&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt;. Technology Preview features provide early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Red Hat Integration provides Debezium connectors for capturing changes from the following databases:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt; &lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;MySQL Connector&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;PostgreSQL Connector&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;MongoDB Connector&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400;"&gt;&lt;span style="font-weight: 400;"&gt;SQL Server Connector&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Debezium connectors are based on the popular&lt;/span&gt;&lt;a href="https://kafka.apache.org/documentation.html#connect"&gt; &lt;span style="font-weight: 400;"&gt;Apache Kafka Connect API&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; and are suitable to be deployed along &lt;/span&gt;&lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;&lt;span style="font-weight: 400;"&gt; &lt;/span&gt;&lt;span style="font-weight: 400;"&gt;Red Hat AMQ Streams&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; Kafka clusters. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;AMQ Streams is a Red Hat Integration component that provides Red Hat’s distribution of Apache Kafka and the popular &lt;/span&gt;&lt;a href="https://www.cncf.io/sandbox-projects/"&gt;&lt;span style="font-weight: 400;"&gt;CNCF sandbox&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; project &lt;/span&gt;&lt;a href="https://strimzi.io/"&gt;&lt;span style="font-weight: 400;"&gt;Strimzi&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt;. AMQ Streams makes running and managing Kafka an OpenShift-native experience&lt;/span&gt;&lt;span style="font-weight: 400;"&gt; by delivering OpenShift Operators, which provide a simplified and automated way to deploy, manage, upgrade, and configure a Kafka ecosystem on OpenShift.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;With this addition, Red Hat Integration now makes available more components to connect systems along the whole enterprise ecosystem. Along with &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/fuse/connectors"&gt;&lt;span style="font-weight: 400;"&gt;Apache Camel’s 200 connectors&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt;, users can connect to practically everything—from legacy systems to Software-as-a-service (SaaS) applications, and application programming interfaces (APIs) to Internet of Things (IoT) devices.&lt;/span&gt;&lt;/p&gt; &lt;h3&gt;&lt;span style="font-weight: 400;"&gt;What is Change Data Capture (CDC)?&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Change Data Capture, or CDC, is a well-established software design pattern for a system that monitors and captures the changes in data so that other software can respond to those changes. CDC captures row-level changes to database tables and passes corresponding change events to a data streaming bus. Applications can read these change event streams and access the change events in the order in which they occurred.&lt;/span&gt;&lt;/p&gt; &lt;h3&gt;&lt;span style="font-weight: 400;"&gt;What is Debezium?&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;&lt;a href="https://debezium.io/"&gt;&lt;span style="font-weight: 400;"&gt;Debezium&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; is a set of distributed services that captures row-level changes in databases so that applications can see and respond to those changes. Debezium connectors record all events to a Red Hat AMQ Streams Kafka cluster, and applications consume those events through AMQ Streams.&lt;/span&gt;&lt;/p&gt; &lt;h3&gt;&lt;span style="font-weight: 400;"&gt;CDC in action with Debezium&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;You can check Sadhana Nandakumar’s  &lt;/span&gt;&lt;a href="https://developers.redhat.com/blog/2019/09/03/cdc-pipeline-with-red-hat-amq-streams-and-red-hat-fuse/"&gt;&lt;span style="font-weight: 400;"&gt;blog post&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; in which she explains how to make use of Red Hat Integration to create a complete CDC pipeline. In the example she captures the changes as they occur using Debezium and streams them using Red Hat AMQ Streams. Then, she filters and transforms the data using Red Hat Fuse and sends it to Elasticsearch, where the data can be further analyzed or used by downstream systems.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;You can download the Red Hat Integration Debezium CDC Technical Preview connectors from the &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/amq/download"&gt;&lt;span style="font-weight: 400;"&gt;Red Hat Developer site&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt;. If you have requests or questions related to running the Debezium Technical Preview, please let us know by sending an email to the &lt;/span&gt;&lt;a href="mailto:debezium-cdc-preview@redhat.com"&gt;&lt;span style="font-weight: 400;"&gt;debezium-cdc-preview mailing list&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt;. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#38;linkname=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fred-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview%2F&amp;#038;title=Red%20Hat%20advances%20Debezium%20CDC%20connectors%20for%20Apache%20Kafka%20support%20to%20Technical%20Preview" data-a2a-url="https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/" data-a2a-title="Red Hat advances Debezium CDC connectors for Apache Kafka support to Technical Preview"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/"&gt;Red Hat advances Debezium CDC connectors for Apache Kafka support to Technical Preview&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/K4vla7dkDoQ" height="1" width="1" alt=""/&gt;</content><summary>After a couple of months in Developer Preview, the Debezium Apache Kafka connectors for change data capture (CDC) are now available as a Technical Preview as part of the Q4 release of Red Hat Integration. Technology Preview features provide early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. Red Hat Integration provi...</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2019-11-22T08:05:18Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/</feedburner:origLink></entry><entry><title>Cloud-native integration with Kubernetes and Camel K</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/AzTPzACP1HY/" /><category term="Camel K" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="devnation" scheme="searchisko:content:tags" /><category term="events" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Editorial Team</name></author><id>searchisko:content:id:jbossorg_blog-cloud_native_integration_with_kubernetes_and_camel_k</id><updated>2019-11-22T08:00:57Z</updated><published>2019-11-22T08:00:57Z</published><content type="html">&lt;p&gt;Our first &lt;a href="https://developers.redhat.com/devnationlive-india/"&gt;DevNation Live regional event was held in Bengaluru, India&lt;/a&gt; in July. This free technology event focused on open source innovations, with sessions presented by elite Red Hat technologists.&lt;/p&gt; &lt;p&gt;In this session, &lt;a href="https://developers.redhat.com/blog/author/kameshsampath/"&gt;Kamesh Sampath&lt;/a&gt; shows how to apply common Enterprise Integration Patterns (EIP) with Apache Camel, &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes, and Red Hat OpenShift&lt;/a&gt;. You will see how the new Camel K framework helps in deploying Camel DSL code as &amp;#8220;integrations&amp;#8221; in Kubernetes/OpenShift.&lt;span id="more-624787"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Watch the complete presentation:&lt;/p&gt; &lt;p&gt;&lt;iframe src="https://www.youtube.com/embed/4Xuax4s-LIc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h3&gt;Learn more&lt;/h3&gt; &lt;p&gt;Join us at an upcoming&lt;a href="https://developers.redhat.com/events/"&gt; developer event&lt;/a&gt;, and see our collection of&lt;a href="https://developers.redhat.com/devnation/?page=0"&gt; past DevNation Live tech talks&lt;/a&gt;&lt;a href="https://developers.redhat.com/events/"&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#38;linkname=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F22%2Fcloud-native-integration-with-kubernetes-and-camel-k%2F&amp;#038;title=Cloud-native%20integration%20with%20Kubernetes%20and%20Camel%20K" data-a2a-url="https://developers.redhat.com/blog/2019/11/22/cloud-native-integration-with-kubernetes-and-camel-k/" data-a2a-title="Cloud-native integration with Kubernetes and Camel K"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/22/cloud-native-integration-with-kubernetes-and-camel-k/"&gt;Cloud-native integration with Kubernetes and Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/AzTPzACP1HY" height="1" width="1" alt=""/&gt;</content><summary>Our first DevNation Live regional event was held in Bengaluru, India in July. This free technology event focused on open source innovations, with sessions presented by elite Red Hat technologists. In this session, Kamesh Sampath shows how to apply common Enterprise Integration Patterns (EIP) with Apache Camel, Kubernetes, and Red Hat OpenShift. You will see how the new Camel K framework helps in d...</summary><dc:creator>Editorial Team</dc:creator><dc:date>2019-11-22T08:00:57Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/22/cloud-native-integration-with-kubernetes-and-camel-k/</feedburner:origLink></entry><entry><title>This Week in JBoss, 21th November 2019 - JCliff: Wildfly under Ansible control!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lyWmLMKe3E8/this-week-in-jboss-21th-november-2019-jcliff-wildfly-under-ansible-control" /><category term="AMQ" scheme="searchisko:content:tags" /><category term="ansible" scheme="searchisko:content:tags" /><category term="byteman" scheme="searchisko:content:tags" /><category term="Devoxx" scheme="searchisko:content:tags" /><category term="EAP" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="infinispan" scheme="searchisko:content:tags" /><category term="jboss-tools" scheme="searchisko:content:tags" /><category term="jcliff" scheme="searchisko:content:tags" /><category term="keycloak" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="narayana" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Operators" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="Red Hat Fuse" scheme="searchisko:content:tags" /><author><name>Romain Pelisse</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_21th_november_2019_jcliff_wildfly_under_ansible_control</id><updated>2019-11-21T18:35:28Z</updated><published>2019-11-21T18:35:28Z</published><content type="html">&lt;!-- [DocumentBodyStart:d1d3494f-7f60-4fff-b6d2-a3843ca0d12b] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;&lt;em&gt;Our last editorial was all about Quarkus, the project having just released its version 1.0. Of course, this issue will still feature of lot of news about the latest and brightest baby of the JBoss community. But I also wanted to bring up again a project have been heavily involved: JCliff and its Ansible integration. And I'm going to shamelessly used this editorial to promote it a bit &lt;span aria-label="Happy" class="emoticon_happy emoticon-inline" style="height:16px;width:16px;"&gt;&lt;/span&gt; !&lt;br/&gt;&lt;/em&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/10/Java-Jcliff.jpg"&gt;&lt;img alt="" class="image-1 jive-image" src="https://developers.redhat.com/blog/wp-content/uploads/2019/10/Java-Jcliff.jpg" style="width: 620px; height: 343px; display: block; margin-left: auto; margin-right: auto;"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;JCliff - Putting Wildfly under Ansible control&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;So what is JCliff? JCliff is a small Java tool written to help integrate Wildfly into Puppet. It&amp;#8217;s basically a layer between the JBoss CLI and the configuration management tool. Indeed, Puppet, like Ansible are working on &lt;strong&gt;state&lt;/strong&gt;. They both check that the target, in this case an instance of Widfly, is in the correct state. If not, the tool will correct the issue and ensure the system is in the proper state. JCliff simply turn the question &amp;ldquo;is this in the appropriate state&amp;#8221; into a series of JBoss CLI queries. It also does the same when the configuration management tool asks to correct the state. In the last year, we&amp;#8217;ve worked hard into integrating JCliff inside Ansible, so people using it, can be fine-tuned and automated, as much as possible, their Wildfly configuration and deployment. Please, checkout our article on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/06/managing-jboss-eap-wildfly-using-jcliff/" rel="nofollow"&gt;Managing JBoss EAP/Wildfly using Jcliff&lt;/a&gt;, if you want to know more about it!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p style="font-size: 0.9rem; font-style: italic;"&gt;&lt;a href="https://live.staticflickr.com/3345/3190647471_38d04ef9f5.jpg"&gt;&lt;img alt="Reflection Nebula NGC 1999" src="https://live.staticflickr.com/3345/3190647471_38d04ef9f5.jpg" style="display: block; margin-left: auto; margin-right: auto;"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="font-size: 0.9rem; font-style: italic; text-align: center;"&gt;&lt;a class="jive-link-external-small" href="https://www.flickr.com/photos/34168865@N08/3190647471" rel="nofollow"&gt;"Reflection Nebula NGC 1999"&lt;/a&gt;&lt;span&gt; by &lt;a class="jive-link-external-small" href="https://www.flickr.com/photos/34168865@N08" rel="nofollow"&gt;Hubble Heritage&lt;/a&gt;&lt;/span&gt; is licensed under &lt;a class="jive-link-external-small" href="https://creativecommons.org/licenses/by-sa/2.0/?ref=ccsearch&amp;amp;atype=html" rel="nofollow"&gt;CC BY-SA 2.0&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px; text-align: center;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Quarkus&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;While I'm (rightfully) proud of our integration of Wildfly for Ansible, the fact remains that the current star of the JBoss ecosystem is, without a doubt, Quarkus. You don't have to take our word for it, check out &lt;a class="jive-link-external-small" href="https://twitter.com/burrsutter/status/1197223667635240960" rel="nofollow"&gt;Thoughtswork thinks about Quarkus&lt;/a&gt;! If you have not yet checked out Quarkus, the recent &lt;a class="jive-link-external-small" href="https://quarkus.io/blog/announcing-quarkus-1-0/" rel="nofollow"&gt;release of the 1.0&lt;/a&gt; is the perfect opportunity to do so.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Why should you? Because any web Java developer or JEE developer needs to! By the way, if you are looking for an easy entry point, just follow this tutorial on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/07/quarkus-modernize-helloworld-jboss-eap-quickstart-part-1/" rel="nofollow"&gt;Quarkus: Modernize "helloworld" JBoss EAP quickstart, Part 1&lt;/a&gt;&lt;/p&gt;&lt;p&gt;and its follow-up &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/08/quarkus-modernize-helloworld-jboss-eap-quickstart-part-2/" rel="nofollow"&gt;Quarkus: Modernize "helloworld" JBoss EAP quickstart, Part 2.&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you are already on board with Quarkus, then maybe take a look to this recent article on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/18/how-quarkus-brings-imperative-and-reactive-programming-together/" rel="nofollow"&gt;How Quarkus brings imperative and reactive programming together&lt;/a&gt;, I'm pretty sure you might find it interesting.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Kubernetes&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The last two weeks have seen a lot of interesting content about Kubernetes being released! The first one that caught our eyes is this one on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/12/plumbing-kubernetes-ci-cd-with-tekton/" rel="nofollow"&gt;Plumbing Kubernetes CI/CD with Tekton.&lt;/a&gt; Another one worth mentioning is this article on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/12/using-the-red-hat-openshift-tuned-operator-for-elasticsearch/" rel="nofollow"&gt;Using the Red Hat OpenShift tuned Operator for Elasticsearch. &lt;/a&gt;Both are quite intriguing and discuss some very cool use cases.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Techbytes&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Enough about Quarkus and Kubernetes for now, let's take a look at what else the JBoss community has been up to! First all, let us recommend to you this article on &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/14/tracing-kubernetes-applications-with-jaeger-and-eclipse-che/" rel="nofollow"&gt;Tracing Kubernetes applications with Jaeger and Eclipse Che&lt;/a&gt;, because this kind of technique might be quite handy someday and it's a good read. Next, less "debug-oriented" and more "let's do cool things", comes this other article on&amp;#160; &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/11/autoscaling-red-hat-fuse-applications-with-openshift/" rel="nofollow"&gt;OpenShift autoscaling Red Hat Fuse&lt;/a&gt; followed closely by &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2019/11/21/event-based-microservices-with-red-hat-amq-streams/" rel="nofollow"&gt;Event-based microservices with Red Hat AMQ Streams.&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Evangelist's Corner&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;As always, our very own Eric D. Schabell has been releasing material in the past few weeks. Noteworthy is his webinar on &lt;a class="jive-link-external-small" href="http://www.schabell.org/2019/11/blueprint-for-omnichannel-integration-architecture-webinar.html" rel="nofollow"&gt;Blueprint for omnichannel integration architecture&lt;/a&gt;, but also his tutorial on &lt;a class="jive-link-external-small" href="https://www.schabell.org/2019/11/how-to-setup-openshift-container-platform-in-minutes.html" rel="nofollow"&gt;How to set up OpenShift Container Platform on your local machine in minutes.&lt;/a&gt; Also worth mentioning in this section is the &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/kogito_deep_dive_video_from_devoxx" rel="nofollow"&gt;Kogito deep dive video from Devoxx.&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Releases, releases, releases...&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://www.keycloak.org/2019/11/keycloak-800-released.html" rel="nofollow"&gt;Keycloak - Blog - Keycloak 8.0.0 released&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://infinispan.org/blog/2019/11/18/infinispan-1010beta1/" rel="nofollow"&gt;Infinispan 10.1.0.Beta1 - Infinispan&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="http://bytemanblog.blogspot.com/2019/11/byteman-409-has-been-released.html" rel="nofollow"&gt;Byteman Blog: Byteman 4.0.9 has been released&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="http://jbossts.blogspot.com/2019/11/narayana-5100final-released.html" rel="nofollow"&gt;Narayana team blog: Narayana 5.10.0.Final released&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="http://tools.jboss.org/documentation/whatsnew/jbosstools/4.13.0.Final.html" rel="nofollow"&gt;JBoss Tools 4.13.0&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://developers.redhat.com/products/codeready-studio/overview" rel="nofollow"&gt;Red Hat CodeReady Studio 12.13&lt;/a&gt; for Eclipse 2019-09&lt;/li&gt;&lt;/ul&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;D&amp;eacute;caf&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Enough about Java stuff? Want to hear about JBoss community-related news outside of the Javasphere? Well, did you hear about this supercool integration between Wildfly and Ansible using something called JCliff? OK, enough shameless plug. Even I can see this one is just too much&amp;hellip;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;em&gt;That's all for another edition of the JBoss Editorial, please join us again for more exciting development from the JBoss Communities.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:d1d3494f-7f60-4fff-b6d2-a3843ca0d12b] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lyWmLMKe3E8" height="1" width="1" alt=""/&gt;</content><summary>Our last editorial was all about Quarkus, the project having just released its version 1.0. Of course, this issue will still feature of lot of news about the latest and brightest baby of the JBoss community. But I also wanted to bring up again a project have been heavily involved: JCliff and its Ansible integration. And I'm going to shamelessly used this editorial to promote it a bit !     JCliff ...</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2019-11-21T18:35:28Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2019/11/21/this-week-in-jboss-21th-november-2019-jcliff-wildfly-under-ansible-control</feedburner:origLink></entry><entry><title>Event-based microservices with Red Hat AMQ Streams</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/e2nifu1IK9I/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat AMQ Streams" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Application Runtimes" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>chgan</name></author><id>searchisko:content:id:jbossorg_blog-event_based_microservices_with_red_hat_amq_streams</id><updated>2019-11-21T08:00:38Z</updated><published>2019-11-21T08:00:38Z</published><content type="html">&lt;p&gt;As part of &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/amq" target="_blank" rel="noopener noreferrer"&gt;Red Hat&amp;#8217;s AMQ&lt;/a&gt; offerings, Red Hat offers a Kafka-based event streaming solution both for traditional deployment and microservices-based deployment branded as &lt;a href="https://developers.redhat.com/blog/2019/10/03/deploy-red-hat-amq-streams-and-fuse-on-openshift-container-platform-4/"&gt;Red Hat AMQ Streams&lt;/a&gt;. The &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; AMQ Streams deployment option is based on &lt;a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer"&gt;Strimzi,&lt;/a&gt; an open source tool that makes Kafka deployment as a container on a Kubernetes platform easy because most of the deployment prerequisites are automated with the OpenShift &lt;a href="https://www.redhat.com/en/blog/introducing-operator-framework-building-apps-kubernetes"&gt;Operator Framework&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, we look at how to deploy Apache Kafka on Red Hat OpenShift 4, using reasonable sample microservice applications to showcase the endless possibility of innovation brought by OpenShift and Kafka.&lt;span id="more-650207"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;img class="wp-image-650267 size-large aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screenshot-2019-11-08-at-11.52.29-AM-1024x540.png" alt="AMQ Streams on OpenShift" width="640" height="338" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screenshot-2019-11-08-at-11.52.29-AM-1024x540.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screenshot-2019-11-08-at-11.52.29-AM-300x158.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screenshot-2019-11-08-at-11.52.29-AM-768x405.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p style="text-align: center;"&gt;&lt;em&gt;Figure 1: Our deployment environment.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the environment we will deploy on OpenShift with a number of microservices, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;Account Balance Service&lt;/strong&gt; provides information on the account balance, which has its own MongoDB database services.&lt;/li&gt; &lt;li&gt;The&lt;strong&gt; Credit Service&lt;/strong&gt; performs credit transfer between accounts, storing the credit data in the &lt;code&gt;credit&lt;/code&gt; Kafka topic.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Event Correlator &lt;/strong&gt;listens to the &lt;code&gt;credit&lt;/code&gt; topic, performs the necessary adjustment to the account balance, and updates the changes to the Account Balance Service via the REST API. At the same time, it sends the outcome of this process as a credit response to the Kafka topic &lt;code&gt;credit-response&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MongoDB Kafka Connect&lt;/strong&gt; listens to the content in the &lt;code&gt;credit-response&lt;/code&gt; topic and streams this information to the &lt;strong&gt;Credit Response DB&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Install AMQ Streams on Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;AMQ Streams installation is pretty straightforward on Red Hat OpenShift. The only issue I faced was configuring MongoDB Kafka Connect, and that was mostly due to a lack of detailed documentation and a bug in Kafka Connect. All of these issues are now structurally documented as the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download the YAML installation files from the &lt;a href="https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?downloadType=distributions&amp;#38;product=jboss.amq.streams" target="_blank" rel="noopener noreferrer"&gt;Red Hat Access website&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Note: &lt;/strong&gt;We are installing these AMQ Streams using cluster admin. AMQ Streams includes several custom resources. By default, permission to create, edit, and delete these resources is limited to OpenShift cluster administrators. If you want to allow non-cluster administrators to manage AMQ Streams resources, you must assign them the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift/getting-started-str#assembly-getting-started-strimzi-admin-str" target="_blank" rel="noopener noreferrer"&gt;Strimzi Administrator role&lt;/a&gt;.&lt;/p&gt; &lt;ol start="2"&gt; &lt;li&gt;Deploy the Kafka cluster using the Kafka Operator, which can watch Kafka resources for single and multiple namespaces. In our case, we deploy the Operator to watch for a single namespace. Once you download and unzip the installation files, navigate to the root folder, which contains two folders: &lt;code&gt;examples&lt;/code&gt; and &lt;code&gt;install&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Run the following from the command line to make changes to the provided YAML files for our single OpenShift namespace deployment:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;On Linux:&lt;/p&gt; &lt;pre&gt;$ sed -i 's/namespace: .*/namespace: my-kafka-example/' install/cluster-operator/*RoleBinding*.yaml&lt;/pre&gt; &lt;p&gt;On macOS:&lt;/p&gt; &lt;pre&gt;$ sed -i '' 's/namespace: .*/namespace: my-kafka-example/' install/cluster-operator/*RoleBinding*.yaml&lt;/pre&gt; &lt;ol start="4"&gt; &lt;li&gt;Run the following command to deploy the Operator once the namespace is changed:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc apply -f install/cluster-operator -n my-kafka-example&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can also configure the Kafka Operator to watch for all namespaces. Please refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift/getting-started-str#deploying-cluster-operator-to-watch-whole-cluster-str" target="_blank" rel="noopener noreferrer"&gt;documentation&lt;/a&gt; for details.&lt;/p&gt; &lt;ol start="5"&gt; &lt;li&gt;Deploy the Kafka cluster once the Kafka Operator is deployed. There are two options: ephemeral and persistent. We will deploy a persistent Kafka cluster. Begin by opening the &lt;code&gt;examples/kafka/kafka-persistent.yaml&lt;/code&gt; file and changing the Kafka cluster name in the &lt;code&gt;Kafka.metadata.name&lt;/code&gt; property as follows:&lt;/li&gt; &lt;/ol&gt; &lt;div&gt; &lt;pre style="padding-left: 40px;"&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata:  name: my-kafka-cluster # ...&lt;/pre&gt; &lt;ol start="6"&gt; &lt;li&gt;Configure the Topic Operator as the following in the same &lt;code&gt;kafka-persistent.yaml&lt;/code&gt; file as before, in order to enable auto-creation of the Kafka topics configured in the applications:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;entityOperator:    topicOperator:      watchedNamespace: my-kafka-example      reconciliationIntervalSeconds: 90      zookeeperSessionTimeoutSeconds: 20      topicMetadataMaxAttempts: 6       image: registry.redhat.io/amq7/amq-streams-operator:1.3.0&lt;/pre&gt; &lt;ol start="7"&gt; &lt;li&gt;Run the following command to deploy the Kafka cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;oc apply -f examples/kafka/kafka-persistent.yaml&lt;/pre&gt; &lt;ol start="8"&gt; &lt;li&gt;Deploy and run the following sample Kafka producer:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;oc run kafka-producer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list kafka-cluster-kafka-bootstrap:9092 --topic my-topic&lt;/pre&gt; &lt;ol start="9"&gt; &lt;li&gt;Deploy and run the following sample Kafka consumer:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server kafka-cluster-kafka-bootstrap:9092 --topic my-topic --from-beginning&lt;/pre&gt; &lt;ol start="10"&gt; &lt;li&gt;Verify that the Kafka cluster is working as expected once all of the pods and resources are ready, which means seeing if you can send messages from the producer to the consumer.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Deploy sample application dependencies&lt;/h2&gt; &lt;p&gt;Due to AMQ Streams, brokers are not accessible directly outside of the namespace where they are deployed. We will deploy all of our sample applications in the same namespace as the Kafka cluster. This issue is the same for all Kafka brokers deployed as containers, and this is not a limitation of OpenShift. All external connections are handled and routed by the Kafka bootstrap component.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you wish to access the brokers externally, please refer to this &lt;a href="https://developers.redhat.com/blog/2019/06/10/accessing-apache-kafka-in-strimzi-part-3-red-hat-openshift-routes/"&gt;article&lt;/a&gt; on how to enable this option.&lt;/p&gt; &lt;h3&gt;Deploy MongoDB&lt;/h3&gt; &lt;div&gt; &lt;div&gt;This is our Credit Response database. Run the following command to deploy MongoDB using the provided template:&lt;/div&gt; &lt;pre&gt;oc new-app -f https://raw.githubusercontent.com/chengkuangan/creditresponsemongodb/master/mongodb-deployment-template.yaml&lt;/pre&gt; &lt;p&gt;During the time this article was written and due to the hardcoded database authentication source, I received the following error (&lt;code&gt;source='admin'&lt;/code&gt;) in the Kafka Connect container log when Kafka Connect was trying to send data to MongoDB:&lt;/p&gt; &lt;pre&gt;2019-11-07 12:23:07,617 INFO Cluster created with settings {hosts=[creditresponse:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500} (org.mongodb.driver.cluster) [task-thread-mongodb-sink-0] 2019-11-07 12:23:07,842 INFO Cluster description not yet available. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster) [task-thread-mongodb-sink-0] 2019-11-07 12:23:07,861 INFO Opened connection [connectionId{localValue:1, serverValue:220}] to creditresponse:27017 (org.mongodb.driver.connection) [cluster-ClusterId{value='5dc40cab2efd9074c7742e33', description='null'}-creditresponse:27017] 2019-11-07 12:23:07,863 INFO Monitor thread successfully connected to server with description ServerDescription{address=creditresponse:27017, type=STANDALONE, state=CONNECTED, ok=true, version=ServerVersion{versionList=[3, 2, 10]}, minWireVersion=0, maxWireVersion=4, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=null, roundTripTimeNanos=1014605} (org.mongodb.driver.cluster) [cluster-ClusterId{value='5dc40cab2efd9074c7742e33', description='null'}-creditresponse:27017] 2019-11-07 12:23:07,892 INFO Closed connection [connectionId{localValue:2}] to creditresponse:27017 because there was a socket exception raised by this connection. (org.mongodb.driver.connection) [task-thread-mongodb-sink-0] 2019-11-07 12:23:07,894 ERROR Error on mongodb operation (com.mongodb.kafka.connect.sink.MongoSinkTask) [task-thread-mongodb-sink-0] com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=SCRAM-SHA-1, userName='creditresponse', source='admin', password=&amp;#60;hidden&amp;#62;, mechanismProperties={}}   at com.mongodb.internal.connection.SaslAuthenticator.wrapException(SaslAuthenticator.java:173)&lt;/pre&gt; &lt;p&gt;To work around this problem, perform &lt;code&gt;oc rsh&lt;/code&gt; into the MongoDB pod to create a new &lt;code&gt;creditresponse&lt;/code&gt; user account with the following details:&lt;/p&gt; &lt;pre&gt;mongo --port 27017 -u admin -p creditresponse --authenticationDatabase admin use admin db.runCommand({createRole:"listDatabases",privileges:[{resource:{cluster:true}, actions:["listDatabases"]}],roles:[]}) db.createUser({ "user" : "creditresponse", "pwd" : "creditresponse",    "roles" : [        {            "role" : "listDatabases",            "db" : "admin"        },        {            "role" : "readWrite",            "db" : "creditresponse"        },        {            "role" : "read",            "db" : "local"        }    ] })&lt;/pre&gt; &lt;/div&gt; &lt;h3&gt;Deploy and configure MongoDB Kafka Connect&lt;/h3&gt; &lt;p&gt;AMQ Streams Kafka Connect only comes with &lt;code&gt;FileStreamSourceConnector&lt;/code&gt; and &lt;code&gt;FileStreamSinkConnector&lt;/code&gt;. In order to deploy MongoDB Kafka Connect, we need to build the container image with the MongoDB Kafka Connect JAR file and the Red Hat-supported AMQ Streams base image:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Proceed to create the necessary Dockerfile with the following content:&lt;/li&gt; &lt;/ol&gt; &lt;div&gt; &lt;pre style="padding-left: 40px;"&gt; FROM registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 USER root:root COPY ./mongo-plugins/ /opt/kafka/plugins/ USER kafka:kafka&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Download the MongoDB Kafka Connect JAR files from the &lt;a href="https://www.mongodb.com/kafka-connector" target="_blank" rel="noopener noreferrer"&gt;MongoDB website&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Unzip and copy the JAR file to the &lt;code&gt;mongo-plugins&lt;/code&gt; folder.&lt;/li&gt; &lt;li&gt;Make sure you have a valid Red Hat account in order to log in and access &lt;code&gt;registry.redhat.io&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Build the image:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;docker login registry.redhat.io docker build -t chengkuan/amq-streams-kafka-connect-23:1.3.0&lt;/pre&gt; &lt;ol start="6"&gt; &lt;li&gt;Change &lt;code&gt;kafka-connect.yaml&lt;/code&gt; with the following &lt;code&gt;spec.image&lt;/code&gt;,&lt;em&gt; &lt;code&gt;spec.bootstrapServers&lt;/code&gt;&lt;/em&gt;, and &lt;code&gt;spec.tls.trustedCertificates.secretName&lt;/code&gt;. Take note that the port number for &lt;em&gt;&lt;code&gt;spec.bootstrapServers&lt;/code&gt;&lt;/em&gt; is 9093, which is the default &lt;code&gt;clienttls&lt;/code&gt; port:&lt;/li&gt; &lt;/ol&gt; &lt;div&gt; &lt;pre style="padding-left: 40px;"&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaConnect metadata:  name: mongodb-connect-cluster spec:  version: 2.3.0  replicas: 1  bootstrapServers: kafka-cluster-kafka-bootstrap:9093  tls:    trustedCertificates:      - secretName: kafka-cluster-cluster-ca-cert        certificate: ca.crt   image: docker.io/chengkuan/amq-streams-kafka-connect-23:1.3.0&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;ol start="7"&gt; &lt;li&gt;Deploy Kafka Connect:&lt;/li&gt; &lt;/ol&gt; &lt;div&gt; &lt;pre style="padding-left: 40px;"&gt;oc apply -f examples/kafka-connect/kafka-connect.yaml&lt;/pre&gt; &lt;div&gt; &lt;ol start="8"&gt; &lt;li&gt;Port forward from a local PC to OpenShift&amp;#8217;s connect API service because Kafka Connect&amp;#8217;s pod is not accessible externally:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;oc port-forward service/mongodb-connect-cluster-connect-api 8083:8083&lt;/pre&gt; &lt;ol start="9"&gt; &lt;li&gt;Run the following using &lt;code&gt;curl&lt;/code&gt; or a web browser to verify that the MongoDB Connect plugin loaded successfully:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;curl http://localhost:8083/connector-plugins&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;You will see that the MongoDB Connect plugin is listed:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;[{"class":"com.mongodb.kafka.connect.MongoSinkConnector","type":"sink","version":"0.2"},{"class":"com.mongodb.kafka.connect.MongoSourceConnector","type":"source","version":"0.2"},{"class":"org.apache.kafka.connect.file.FileStreamSinkConnector","type":"sink","version":"2.3.0.redhat-00003"},{"class":"org.apache.kafka.connect.file.FileStreamSourceConnector","type":"source","version":"2.3.0.redhat-00003"}]&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;To configure MongoDB Kafka Connect, download &lt;a href="https://raw.githubusercontent.com/chengkuangan/creditresponsemongodb/master/connect-mongodb-sink.json" target="_blank" rel="noopener noreferrer"&gt;&lt;code&gt;connect-mongodb-sink.json&lt;/code&gt;&lt;/a&gt; and modify the following accordingly:&lt;/p&gt; &lt;pre&gt;{ "name": "mongodb-sink", "config": { "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector", "tasks.max": 1, "topics": "credit-response", "connection.uri": "mongodb://creditresponse:creditresponse@creditresponse:27017", "database": "creditresponse", "collection": "response", "key.converter": "org.apache.kafka.connect.json.JsonConverter", "key.converter.schemas.enable": false, "value.converter": "org.apache.kafka.connect.json.JsonConverter", "value.converter.schemas.enable": false, "max.num.retries": 3 } }&lt;/pre&gt; &lt;p&gt;Next, post to the Kafka Connect REST API:&lt;/p&gt; &lt;pre&gt;curl -d connect-mongodb-sink.json -H "Content-Type: application/json" -X POST http://localhost:8083/connectors&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; At the time this was written, I could not &lt;code&gt;POST&lt;/code&gt; the content successfully to Kafka Connect using the &lt;code&gt;curl&lt;/code&gt; command due to a JSON formatting error. However, I can &lt;code&gt;POST&lt;/code&gt; the same content without errors using Postman.&lt;/p&gt; &lt;/div&gt; &lt;p&gt;Finally, run the following &lt;code&gt;curl&lt;/code&gt; command to verify the configuration:&lt;/p&gt; &lt;/div&gt; &lt;pre&gt;curl http://localhost:8083/connectors/mongodb-sink&lt;/pre&gt; &lt;p&gt;The result:&lt;/p&gt; &lt;pre&gt;{ "name": "mongodb-sink", "config": { "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector", "key.converter.schemas.enable": "false", "database": "creditresponse", "tasks.max": "1", "topics": "credit-response", "max.num.retries": "3", "connection.uri": "mongodb://creditresponse:creditresponse@creditresponse:27017", "value.converter.schemas.enable": "false", "name": "mongodb-sink", "collection": "response", "value.converter": "org.apache.kafka.connect.json.JsonConverter", "key.converter": "org.apache.kafka.connect.json.JsonConverter" }, "tasks": [ { "connector": "mongodb-sink", "task": 0 } ], "type": "sink" }&lt;/pre&gt; &lt;h3&gt;Deploy the Account Balance Service application&lt;/h3&gt; &lt;p&gt;Run the following command to deploy the pods:&lt;/p&gt; &lt;pre&gt;oc new-app https://raw.githubusercontent.com/chengkuangan/accountbalance/master/templates/deployment-templates.yaml&lt;/pre&gt; &lt;h3&gt;Deploy the Credit Service application&lt;/h3&gt; &lt;p&gt;Run the following command to deploy the Credit Service. Ensure that &lt;code&gt;KAFKA_BOOTSTRAP_SERVER&lt;/code&gt; points to the correct server:&lt;/p&gt; &lt;pre&gt;oc new-app https://raw.githubusercontent.com/chengkuangan/creditservice/master/templates/creditservice.json -p KAFKA_BOOTSTRAP_SERVER=kafka-cluster-kafka-bootstrap:9092&lt;/pre&gt; &lt;h3&gt;Deploy the Event Correlator Service&lt;/h3&gt; &lt;div&gt;Run the following command to deploy the Event Correlator Service. Ensure that &lt;code&gt;KAFKA_BOOTSTRAP_SERVER&lt;/code&gt; points to the correct server:&lt;/div&gt; &lt;pre&gt;oc new-app -f https://raw.githubusercontent.com/chengkuangan/eventcorrelator/master/templates/deployment-templates.yaml -p KAFKA_BOOTSTRAP_SERVER=kafka-cluster-kafka-bootstrap:9092&lt;/pre&gt; &lt;h2&gt;Watch the application in action&lt;/h2&gt; &lt;div&gt;Now, let us use our application. Create account balance records:&lt;/div&gt; &lt;pre&gt;curl -H "Content-Type: application/json" -X POST http://accountbalance-my-kafka-example.apps.demo.ocp.internal/ws/pg/balance -d '{"accountId": "20191108-MY-00000001", "balance": 500.00, "lastUpdatedDate": 1563178274158 }' curl -H "Content-Type: application/json" -X POST http://accountbalance-my-kafka-example.apps.demo.ocp.internal/ws/pg/balance -d '{"accountId": "20191108-MY-00000002", "balance": 700.00, "lastUpdatedDate": 1563178274158 }'&lt;/pre&gt; &lt;p&gt;Query the account balance entries created above:&lt;/p&gt; &lt;pre&gt;curl http://accountbalance-my-kafka-example.apps.demo.ocp.internal/ws/pg/balance/all&lt;/pre&gt; &lt;p&gt;The result:&lt;/p&gt; &lt;pre&gt;[ { "_id":"5dc52069a3c1080001ebd539", "accountId":"20191108-MY-00000001", "balance":500.0, "lastUpdatedDate":1563178274158 }, { "_id":"5dc52076a3c1080001ebd53a", "accountId":"20191108-MY-00000002", "balance":700.0, "lastUpdatedDate":1563178274158 } ]&lt;/pre&gt; &lt;p&gt;Perform a credit transfer:&lt;/p&gt; &lt;pre&gt;curl -H "Content-Type: application/json" -X POST http://creditservice-my-kafka-example.apps.demo.ocp.internal/ws/pg/credits -d '{"amount": 10.50, "sourceAccount": "20191108-MY-00000001", "targetAccount": "20191108-MY-00000002"}'&lt;/pre&gt; &lt;p&gt;Query the balance after the credit transfer:&lt;/p&gt; &lt;pre&gt;curl http://accountbalance-my-kafka-example.apps.demo.ocp.internal/ws/pg/balance/all&lt;/pre&gt; &lt;p&gt;The result:&lt;/p&gt; &lt;pre&gt;[ { "_id":"5dc52069a3c1080001ebd539", "accountId":"20191108-MY-00000001", "balance":489.5,"lastUpdatedDate":1573200543543 }, { "_id":"5dc52076a3c1080001ebd53a", "accountId":"20191108-MY-00000002", "balance":710.5, "lastUpdatedDate":1573200543543 } ]&lt;/pre&gt; &lt;p&gt;Perform &lt;code&gt;oc rsh&lt;/code&gt; into the Credit Response MongoDB. Use &lt;code&gt;db.response.find()&lt;/code&gt; to see that the credit response is captured:&lt;/p&gt; &lt;pre&gt;mongo --port 27017 -u admin -p creditresponse --authenticationDatabase admin &amp;#62;use creditresponse switched to db creditresponse &amp;#62;show collections response &amp;#62;db.response.find()&lt;/pre&gt; &lt;p&gt;Result:&lt;/p&gt; &lt;pre&gt;{ "_id" : ObjectId("5dc523f536d41402601d01a4"), "sourceAccountRecordId" : "5dc52069a3c1080001ebd539", "targetAccountRecordId" : "5dc52076a3c1080001ebd53a", "sourceAccountId" : "20191108-MY-00000001", "targetAccountId" : "20191108-MY-00000002", "sourceAccountBalance" : 489.5, "targetAccountBalance" : 710.5, "creditRecordId" : "ykvlkqk2puzc5u", "creditAmount" : 10.5, "transactionDate" : NumberLong("1573200543543") }&lt;/pre&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift/" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams on OpenShift documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kafka.apache.org/documentation/#connect" target="_blank" rel="noopener noreferrer"&gt;Apache Kafka Connect documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#38;linkname=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F11%2F21%2Fevent-based-microservices-with-red-hat-amq-streams%2F&amp;#038;title=Event-based%20microservices%20with%20Red%20Hat%20AMQ%20Streams" data-a2a-url="https://developers.redhat.com/blog/2019/11/21/event-based-microservices-with-red-hat-amq-streams/" data-a2a-title="Event-based microservices with Red Hat AMQ Streams"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/11/21/event-based-microservices-with-red-hat-amq-streams/"&gt;Event-based microservices with Red Hat AMQ Streams&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/e2nifu1IK9I" height="1" width="1" alt=""/&gt;</content><summary>As part of Red Hat’s AMQ offerings, Red Hat offers a Kafka-based event streaming solution both for traditional deployment and microservices-based deployment branded as Red Hat AMQ Streams. The Red Hat OpenShift AMQ Streams deployment option is based on Strimzi, an open source tool that makes Kafka deployment as a container on a Kubernetes platform easy because most of the deployment prerequisites ...</summary><dc:creator>chgan</dc:creator><dc:date>2019-11-21T08:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/11/21/event-based-microservices-with-red-hat-amq-streams/</feedburner:origLink></entry></feed>
